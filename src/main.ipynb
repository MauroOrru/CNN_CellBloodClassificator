{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mauro\\anaconda3\\envs\\my_ai_env\\lib\\site-packages\\albumentations\\__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.5' (you have '2.0.4'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Librerie personalizzate\n",
    "from dataset import BloodCellDataset\n",
    "from model import CustomModel\n",
    "from train import train_one_epoch, validate\n",
    "from utils import plot_confusion_matrix, plot_normalized_confusion_matrix\n",
    "\n",
    "# Albumentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Parameters \n",
    " Initialization of some hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"\" # Path to the dataset\n",
    "VALID_FOLDERS = [] # To select only some folders from the dataset\n",
    "IMG_SIZE = 400 # Image size: height = width = IMG_SIZE\n",
    "\n",
    "TEST_RATIO = 0.1  # 10% set test\n",
    "N_FOLDS = 5\n",
    "SEED = 42 # Fixed seed value\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20 \n",
    "LR = 1e-4 # Learning rate\n",
    "MODEL_NAME = \"tf_efficientnetv2_b0\" # Choose model from timm library. For example: \"tf_efficientnetv2_b0\" or \"resnet18\"\n",
    "PRETRAINED = False # Use True if you want to use a pretrained model\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "# Set all the seeds for reproducibility\n",
    "random.seed(SEED) # set seed for random module\n",
    "np.random.seed(SEED) # set seed for numpy module\n",
    "torch.manual_seed(SEED) # set seed for torch module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation\n",
    "In this section dataset is created and loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = BloodCellDataset(\n",
    "    root_dir=DATASET_PATH,\n",
    "    valid_folders=VALID_FOLDERS,\n",
    "    img_size=IMG_SIZE,\n",
    "    transform=None  # No transformations\n",
    ")\n",
    "\n",
    "all_labels = np.array(full_dataset.labels) # Return all the labels in the dataset\n",
    "all_indices = np.arange(len(full_dataset)) # Return the indices of all the examples in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset fragmentation\n",
    "A shuffle of all the indexes of the dataset is performed, for each class, and creates the test and train set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set esterno: 3298 esempi\n",
      "Train+Val set (K-fold): 29720 esempi\n"
     ]
    }
   ],
   "source": [
    "class_to_indices = {} \n",
    "# Group the indices of the examples by class and put them in a dictionary class_to_indices\n",
    "for idx, label in enumerate(all_labels):\n",
    "    class_to_indices.setdefault(label, []).append(idx)\n",
    "\n",
    "test_indices = [] # List of indices for the test set\n",
    "trainval_indices = [] # List of indices for the train+val set (for K-fold cross-validation)\n",
    "\n",
    "for label, idx_list in class_to_indices.items():\n",
    "    idx_arr = np.array(idx_list) # Convert the list of indices to a numpy array\n",
    "    np.random.shuffle(idx_arr)  # Shuffle the indices\n",
    "    n_test = int(len(idx_arr)*TEST_RATIO) # Number of test examples for the current class\n",
    "\n",
    "    test_part = idx_arr[:n_test] # Take n_test random indices for the test set\n",
    "    train_part = idx_arr[n_test:]  # Take the remaining indices for the train set\n",
    "\n",
    "    test_indices.extend(test_part) # Add the test indices to the test set\n",
    "    trainval_indices.extend(train_part) # Add the train indices to the train set\n",
    "\n",
    "test_indices = np.array(test_indices)\n",
    "trainval_indices = np.array(trainval_indices)\n",
    "\n",
    "print(f\"Test set esterno: {len(test_indices)} esempi\")\n",
    "print(f\"Train+Val set (K-fold): {len(trainval_indices)} esempi\")\n",
    "\n",
    "final_test_dataset = Subset(full_dataset, test_indices) # Test set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-FOLD\n",
    "Il dataset composto dal 90% degli esempi (stratificato per classe) viene impiegato per il K-Fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "labels_for_90 = all_labels[trainval_indices]\n",
    "\n",
    "# Data augmentation per train\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Rotate(limit=180, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.1),\n",
    "    A.Resize(IMG_SIZE,IMG_SIZE),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Transform di base per val/test\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(IMG_SIZE,IMG_SIZE),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Per la WeightedRandomSampler, calcoliamo le class weights\n",
    "# 1 / freq_di_classe = peso. (Es. meno frequente = peso maggiore)\n",
    "unique_labels, counts = np.unique(labels_for_90, return_counts=True)\n",
    "class_counts = dict(zip(unique_labels, counts))\n",
    "\n",
    "# Ad es. weight[class] = 1/count\n",
    "max_count = max(class_counts.values())\n",
    "weight_per_class = {}\n",
    "for c, cnt in class_counts.items():\n",
    "    weight_per_class[c] = max_count / cnt  # un modo per \"bilanciare\"\n",
    "\n",
    "def make_sampler(indices_array):\n",
    "    # Ritorna un WeightedRandomSampler per oversampling\n",
    "    # in base a weight_per_class.\n",
    "    sample_weights = []\n",
    "    for idx in indices_array:\n",
    "        label = all_labels[idx]\n",
    "        sample_weights.append(weight_per_class[label])\n",
    "    sampler = torch.utils.data.WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(indices_array),  # parità con dimensione set\n",
    "        replacement=True  # sampling con rimpiazzo\n",
    "    )\n",
    "    return sampler\n",
    "\n",
    "# Funzione per creare un dataset con transform\n",
    "class SubsetAlbumentations(Dataset):\n",
    "    def __init__(self, base_dataset, indices, albumentations_transform=None):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.indices = indices\n",
    "        self.transform = albumentations_transform\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    def __getitem__(self, i):\n",
    "        real_idx = self.indices[i]\n",
    "        img, label = self.base_dataset[real_idx]\n",
    "        # Se dataset ritorna un np array HxWxC o un Tensor CxHxW...\n",
    "        # Adattiamo per Albumentations\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img_np = img.permute(1,2,0).numpy()\n",
    "        else:\n",
    "            img_np = img\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=img_np)\n",
    "            img = augmented[\"image\"]\n",
    "        return img, label\n",
    "\n",
    "fold_accuracies = []\n",
    "fold_number = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K FOLD EXEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_idx, val_idx in skf.split(trainval_indices, labels_for_90):\n",
    "    print(f\"\\n=== Fold {fold_number}/{N_FOLDS} ===\")\n",
    "    real_train_indices = trainval_indices[train_idx]\n",
    "    real_val_indices   = trainval_indices[val_idx]\n",
    "\n",
    "    print(\"Train set fold:\", len(real_train_indices), \"| Val set fold:\", len(real_val_indices))\n",
    "\n",
    "    ds_train = SubsetAlbumentations(full_dataset, real_train_indices, albumentations_transform=train_transform)\n",
    "    ds_val   = SubsetAlbumentations(full_dataset, real_val_indices, albumentations_transform=val_transform)\n",
    "\n",
    "    sampler_train = make_sampler(real_train_indices)\n",
    "    dl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, sampler=sampler_train, num_workers=0)\n",
    "    dl_val   = DataLoader(ds_val, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    model = CustomModel(\n",
    "        num_classes=full_dataset.num_classes,\n",
    "        model_name=MODEL_NAME,\n",
    "        pretrained=PRETRAINED\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"  Epoch {epoch+1}/{EPOCHS}\")\n",
    "        train_loss, train_acc = train_one_epoch(model, dl_train, criterion, optimizer, DEVICE)\n",
    "        val_loss, val_acc, precision, recall, f1, preds, labels = validate(model, dl_val, criterion, DEVICE)\n",
    "\n",
    "        print(f\"    TrainLoss: {train_loss:.4f}  Acc: {train_acc:.4f}\"\n",
    "              f\" | ValLoss: {val_loss:.4f}  Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # --- SALVATAGGIO DEL BEST MODEL ---\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), f\"{MODEL_NAME}_best_fold{fold_number}.pth\")\n",
    "            print(f\"    ► Salvo best model per fold {fold_number} con val_acc = {best_val_acc:.4f}\")\n",
    "\n",
    "    fold_accuracies.append(best_val_acc)\n",
    "    print(f\"*** Best Val Acc Fold {fold_number}: {best_val_acc:.4f} ***\")\n",
    "\n",
    "    fold_number += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risultati k-cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_acc = np.mean(fold_accuracies)\n",
    "std_acc = np.std(fold_accuracies)\n",
    "print(f\"\\nRISULTATI CROSS VALIDATION: acc media {mean_acc:.4f} +/- {std_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training su tutto il 90% e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_90 = SubsetAlbumentations(full_dataset, trainval_indices, albumentations_transform=train_transform)\n",
    "sampler_90  = make_sampler(trainval_indices)\n",
    "dl_train_90 = DataLoader(ds_train_90, batch_size=BATCH_SIZE, sampler=sampler_90, num_workers=0)\n",
    "\n",
    "ds_test = SubsetAlbumentations(full_dataset, test_indices, albumentations_transform=val_transform)\n",
    "dl_test = DataLoader(ds_test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "final_model = CustomModel(num_classes=full_dataset.num_classes, model_name=MODEL_NAME, pretrained=PRETRAINED).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== TRAINING FINALE SU TUTTO IL 90% ===\")\n",
    "EPOCHS = 30\n",
    "best_val_acc = 0.0\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(final_model, dl_train_90, criterion, optimizer, DEVICE)\n",
    "    val_loss, val_acc, precision, recall, f1, preds, labels = validate(final_model, dl_test, criterion, DEVICE)\n",
    "\n",
    "    print(f\"    TrainLoss: {train_loss:.4f}  Acc: {train_acc:.4f}\"\n",
    "              f\" | ValLoss: {val_loss:.4f}  Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # --- SALVATAGGIO DEL BEST MODEL ---\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(final_model.state_dict(), f\"{MODEL_NAME}_best_90.pth\")\n",
    "        print(f\"    ► Salvo best model per fold {fold_number} con val_acc = {best_val_acc:.4f}\")\n",
    "\n",
    "torch.save(final_model.state_dict(), f\"{MODEL_NAME}_final_90perc.pth\")\n",
    "print(\"► Salvato il modello addestrato su tutto il 90%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valutazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Crea un’istanza del modello, identico a quello usato in training\n",
    "loaded_model = CustomModel(\n",
    "    num_classes=full_dataset.num_classes,\n",
    "    model_name=MODEL_NAME, \n",
    "    pretrained=False\n",
    ").to(DEVICE)\n",
    "\n",
    "# 2) Carica i pesi salvati\n",
    "loaded_model.load_state_dict(torch.load(\"tf_efficientnetv2_b0_best_90.pth\"))\n",
    "\n",
    "# 3) Metti il modello in eval mode\n",
    "loaded_model.eval()\n",
    "\n",
    "# 4) Valuta sul test set\n",
    "test_loss, test_acc, p, r, f1, preds, labels = validate(\n",
    "    loaded_model, \n",
    "    dl_test,      # il DataLoader del test\n",
    "    criterion,    # ad esempio nn.CrossEntropyLoss()\n",
    "    DEVICE\n",
    ")\n",
    "\n",
    "print(\"=== EVALUATION DEL MODELLO CARICATO ===\")\n",
    "print(f\"Test Loss {test_loss:.4f}, Test Acc {test_acc:.4f}\")\n",
    "print(f\"Precision {p:.2f}, Recall {r:.2f}, F1 {f1:.2f}\")\n",
    "\n",
    "plot_confusion_matrix(labels, preds, full_dataset.classes)\n",
    "plot_normalized_confusion_matrix(labels, preds, full_dataset.classes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
